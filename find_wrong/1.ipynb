{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 모델 초기화\n",
    "model = YOLO('yolov8n.pt')  # Nano 모델 사용\n",
    "\n",
    "# 데이터셋 설정 파일 경로\n",
    "data_path = r'C:\\Users\\kj100\\Downloads\\American Sign Language Letters.v1-v1.yolov8\\data.yaml'  # data.yaml 파일 경로 (YOLO 형식)\n",
    "\n",
    "# 모델 학습\n",
    "model.train(\n",
    "    data=data_path,    # 데이터셋 경로\n",
    "    epochs=10,         # 학습 반복 횟수\n",
    "    imgsz=640,         # 입력 이미지 크기\n",
    "    batch=16,          # 배치 크기\n",
    "    name='asl_yolov8_model'  # 저장 디렉토리 이름\n",
    ")\n",
    "\n",
    "# 학습된 모델 저장\n",
    "model.save('asl_yolov8_model.pt')\n",
    "print(\"YOLOv8 모델 학습 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\n",
      "\n",
      "0: 480x640 (no detections), 145.0ms\n",
      "Speed: 9.0ms preprocess, 145.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 94.0ms\n",
      "Speed: 3.0ms preprocess, 94.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.1ms\n",
      "Speed: 3.0ms preprocess, 74.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 113.1ms\n",
      "Speed: 1.0ms preprocess, 113.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 76.6ms\n",
      "Speed: 2.0ms preprocess, 76.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 113.0ms\n",
      "Speed: 1.0ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 122.0ms\n",
      "Speed: 3.0ms preprocess, 122.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 99.0ms\n",
      "Speed: 2.0ms preprocess, 99.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 O, 88.7ms\n",
      "Speed: 2.0ms preprocess, 88.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 83.2ms\n",
      "Speed: 2.0ms preprocess, 83.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 79.7ms\n",
      "Speed: 1.5ms preprocess, 79.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 90.0ms\n",
      "Speed: 2.0ms preprocess, 90.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 92.2ms\n",
      "Speed: 3.0ms preprocess, 92.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 96.6ms\n",
      "Speed: 3.0ms preprocess, 96.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 74.1ms\n",
      "Speed: 2.0ms preprocess, 74.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 64.7ms\n",
      "Speed: 2.0ms preprocess, 64.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 81.1ms\n",
      "Speed: 1.0ms preprocess, 81.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 91.6ms\n",
      "Speed: 2.0ms preprocess, 91.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 82.6ms\n",
      "Speed: 2.0ms preprocess, 82.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 O, 159.2ms\n",
      "Speed: 3.0ms preprocess, 159.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "인식된 알파벳: O\n",
      "\n",
      "0: 480x640 (no detections), 121.6ms\n",
      "Speed: 2.0ms preprocess, 121.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 101.6ms\n",
      "Speed: 5.0ms preprocess, 101.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 111.0ms\n",
      "Speed: 2.0ms preprocess, 111.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 O, 113.0ms\n",
      "Speed: 3.0ms preprocess, 113.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "인식된 알파벳: O\n",
      "\n",
      "0: 480x640 1 O, 96.7ms\n",
      "Speed: 3.0ms preprocess, 96.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 102.2ms\n",
      "Speed: 2.0ms preprocess, 102.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 116.6ms\n",
      "Speed: 3.0ms preprocess, 116.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 114.6ms\n",
      "Speed: 4.0ms preprocess, 114.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 124.6ms\n",
      "Speed: 8.0ms preprocess, 124.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 113.1ms\n",
      "Speed: 2.0ms preprocess, 113.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 94.6ms\n",
      "Speed: 2.0ms preprocess, 94.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 94.7ms\n",
      "Speed: 2.0ms preprocess, 94.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 O, 95.1ms\n",
      "Speed: 2.0ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('asl_yolov8_model.pt')\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 화면 좌우 반전 (거울 효과)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # YOLOv8 예측\n",
    "    results = model.predict(source=frame, conf=0.5, show=False)  # YOLOv8 예측\n",
    "    detections = results[0].boxes if results[0].boxes else []  # YOLOv8 탐지 결과\n",
    "\n",
    "    # MediaPipe 손 랜드마크 감지\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    # MediaPipe 랜드마크 표시\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    # YOLO 탐지 결과 표시 및 알파벳 출력\n",
    "    detected_alphabet = None  # 인식된 알파벳 초기화\n",
    "    for detection in detections:\n",
    "        coords = detection.xyxy[0].tolist()  # 좌표 정보 리스트로 변환\n",
    "        x1, y1, x2, y2 = map(int, coords[:4])  # 바운딩 박스 좌표\n",
    "        conf = detection.conf[0].item()  # 신뢰도\n",
    "        class_id = int(detection.cls[0].item())  # 클래스 ID\n",
    "\n",
    "        # 신뢰도 기준 추가\n",
    "        if conf >= 0.6:  # 신뢰도가 60% 이상인 경우만 처리\n",
    "            detected_alphabet = model.names[class_id]\n",
    "            label = f\"{detected_alphabet} {conf:.2f}\"\n",
    "\n",
    "            # 바운딩 박스 그리기\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # 콘솔에 인식된 알파벳 출력\n",
    "    if detected_alphabet:\n",
    "        print(f\"인식된 알파벳: {detected_alphabet}\")\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv8 + MediaPipe\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\n",
      "인식된 알파벳: M\n",
      "인식된 알파벳: K\n",
      "인식된 알파벳: I\n",
      "인식된 알파벳: K\n",
      "인식된 알파벳: J\n",
      "인식된 알파벳: Y\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('asl_yolov8_model.pt')\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "# 단어를 형성하기 위한 변수\n",
    "current_alphabet = None\n",
    "word = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 화면 좌우 반전 (거울 효과)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # YOLOv8 예측\n",
    "    results = model.predict(source=frame, conf=0.5, show=False, verbose=False)  # YOLOv8 예측 (로그 숨김)\n",
    "    detections = results[0].boxes if results[0].boxes else []  # YOLOv8 탐지 결과\n",
    "\n",
    "    # YOLO 탐지 결과 표시\n",
    "    detected_alphabet = None\n",
    "    for detection in detections:\n",
    "        coords = detection.xyxy[0].tolist()  # 바운딩 박스 좌표\n",
    "        x1, y1, x2, y2 = map(int, coords[:4])\n",
    "        conf = detection.conf[0].item()\n",
    "        if conf >= 0.6:  # 신뢰도 기준 추가\n",
    "            class_id = int(detection.cls[0].item())  # 클래스 ID\n",
    "            detected_alphabet = model.names[class_id]\n",
    "\n",
    "            # YOLO 바운딩 박스 및 레이블 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            label = f\"{detected_alphabet} {conf:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # MediaPipe 손 랜드마크 감지\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    # MediaPipe 랜드마크 표시\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    # 새 알파벳 감지 시 단어에 추가\n",
    "    if detected_alphabet and detected_alphabet != current_alphabet:\n",
    "        current_alphabet = detected_alphabet\n",
    "        word.append(detected_alphabet)\n",
    "        print(f\"인식된 알파벳: {detected_alphabet}\")\n",
    "\n",
    "        # 단어 완성 여부 확인\n",
    "        if \"\".join(word).lower() == \"hello\":\n",
    "            print(\"완성된 단어: HELLO\")\n",
    "            word = []  # 단어를 초기화하여 다시 시작\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv8 + MediaPipe\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\n",
      "틀렸습니다! 현재 알파벳: None, 감지된 알파벳: F\n",
      "틀렸습니다! 현재 알파벳: F, 감지된 알파벳: Y\n",
      "틀렸습니다! 현재 알파벳: Y, 감지된 알파벳: K\n",
      "틀렸습니다! 현재 알파벳: K, 감지된 알파벳: V\n",
      "틀렸습니다! 현재 알파벳: V, 감지된 알파벳: K\n",
      "틀렸습니다! 현재 알파벳: K, 감지된 알파벳: F\n",
      "틀렸습니다! 현재 알파벳: F, 감지된 알파벳: K\n",
      "틀렸습니다! 현재 알파벳: K, 감지된 알파벳: F\n",
      "틀렸습니다! 현재 알파벳: F, 감지된 알파벳: K\n",
      "틀렸습니다! 현재 알파벳: K, 감지된 알파벳: F\n",
      "틀렸습니다! 현재 알파벳: F, 감지된 알파벳: K\n",
      "틀렸습니다! 현재 알파벳: K, 감지된 알파벳: F\n",
      "틀렸습니다! 현재 알파벳: F, 감지된 알파벳: K\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('asl_yolov8_model.pt')\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "# 알파벳 비교를 위한 변수\n",
    "current_alphabet = None\n",
    "last_wrong_time = 0  # 틀렸을 때의 타임스탬프\n",
    "wrong_display_duration = 2  # 틀렸을 때 빨간색 상자를 표시할 시간(초)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 화면 좌우 반전 (거울 효과)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # YOLOv8 예측\n",
    "    results = model.predict(source=frame, conf=0.5, show=False, verbose=False)  # YOLOv8 예측 (로그 숨김)\n",
    "    detections = results[0].boxes if results[0].boxes else []  # YOLOv8 탐지 결과\n",
    "\n",
    "    # YOLO 탐지 결과 표시\n",
    "    detected_alphabet = None\n",
    "    for detection in detections:\n",
    "        coords = detection.xyxy[0].tolist()  # 바운딩 박스 좌표\n",
    "        x1, y1, x2, y2 = map(int, coords[:4])\n",
    "        conf = detection.conf[0].item()\n",
    "        if conf >= 0.6:  # 신뢰도 기준 추가\n",
    "            class_id = int(detection.cls[0].item())  # 클래스 ID\n",
    "            detected_alphabet = model.names[class_id]\n",
    "\n",
    "            # 알파벳 비교 및 처리\n",
    "            if detected_alphabet == current_alphabet:\n",
    "                # 동일한 알파벳: 파란색 상자\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                label = f\"Detected: {detected_alphabet}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            else:\n",
    "                # 다른 알파벳: 빨간색 상자\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                label = f\"Wrong: {detected_alphabet if detected_alphabet else 'None'}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                last_wrong_time = time.time()  # 틀린 시간 기록\n",
    "\n",
    "                # 출력문에 추가 정보 표시\n",
    "                print(f\"틀렸습니다! 현재 알파벳: {current_alphabet}, 감지된 알파벳: {detected_alphabet}\")\n",
    "\n",
    "            # 현재 알파벳 업데이트\n",
    "            current_alphabet = detected_alphabet\n",
    "\n",
    "    # MediaPipe 손 랜드마크 감지\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    # MediaPipe 랜드마크 표시\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    # 틀렸을 때 빨간색 상자 지속 시간 처리\n",
    "    if time.time() - last_wrong_time < wrong_display_duration:\n",
    "        cv2.putText(frame, \"Wrong Input!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv8 + MediaPipe\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\n",
      "현재 문장: V\n",
      "현재 문장: VF\n",
      "현재 문장: VFW\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('asl_yolov8_model.pt')\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "# 문장 구성 변수\n",
    "current_alphabet = None\n",
    "sentence = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 화면 좌우 반전 (거울 효과)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # YOLOv8 예측\n",
    "    results = model.predict(source=frame, conf=0.5, show=False, verbose=False)  # YOLOv8 예측 (로그 숨김)\n",
    "    detections = results[0].boxes if results[0].boxes else []  # YOLOv8 탐지 결과\n",
    "\n",
    "    # YOLO 탐지 결과 표시\n",
    "    detected_alphabet = None\n",
    "    for detection in detections:\n",
    "        coords = detection.xyxy[0].tolist()  # 바운딩 박스 좌표\n",
    "        x1, y1, x2, y2 = map(int, coords[:4])\n",
    "        conf = detection.conf[0].item()\n",
    "        if conf >= 0.6:  # 신뢰도 기준 추가\n",
    "            class_id = int(detection.cls[0].item())  # 클래스 ID\n",
    "            detected_alphabet = model.names[class_id]\n",
    "\n",
    "            # YOLO 바운딩 박스 및 레이블 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            label = f\"{detected_alphabet} {conf:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # MediaPipe 손 랜드마크 감지\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    # MediaPipe 랜드마크 표시\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    # 새 알파벳 감지 시 문장에 추가\n",
    "    if detected_alphabet and detected_alphabet != current_alphabet:\n",
    "        current_alphabet = detected_alphabet\n",
    "        if detected_alphabet.lower() == \"clear\":  # 특정 단어로 문장 초기화\n",
    "            sentence = []\n",
    "            print(\"문장이 초기화되었습니다.\")\n",
    "        else:\n",
    "            sentence.append(detected_alphabet)\n",
    "            print(f\"현재 문장: {''.join(sentence)}\")\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv8 + MediaPipe\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오답노트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하는 알파벳: L\n",
      "카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: K\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: K\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n",
      "틀렸습니다! 목표 알파벳: L, 감지된 알파벳: V\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO('asl_yolov8_model.pt')\n",
    "\n",
    "# MediaPipe Hands 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 유저로부터 원하는 알파벳 입력받기\n",
    "target_alphabet = input(\"원하는 알파벳을 입력하세요: \").upper() # 대문자 처리 했고\n",
    "print(f\"원하는 알파벳: {target_alphabet}\")\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"카메라가 실행되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "last_wrong_time = 0  # 틀렸을 때의 타임스탬프\n",
    "wrong_display_duration = 2  # 틀렸을 때 빨간색 상자를 표시할 시간(초)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 화면 좌우 반전 (거울 효과)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # YOLOv8 예측\n",
    "    results = model.predict(source=frame, conf=0.5, show=False, verbose=False)  # YOLOv8 예측 (로그 숨김)\n",
    "    detections = results[0].boxes if results[0].boxes else []  # YOLOv8 탐지 결과\n",
    "\n",
    "    detected_alphabet = None\n",
    "    for detection in detections:\n",
    "        coords = detection.xyxy[0].tolist()  # 바운딩 박스 좌표\n",
    "        x1, y1, x2, y2 = map(int, coords[:4])\n",
    "        conf = detection.conf[0].item()\n",
    "        if conf >= 0.6:  # 신뢰도 기준 추가\n",
    "            class_id = int(detection.cls[0].item())  # 클래스 ID\n",
    "            detected_alphabet = model.names[class_id].upper()\n",
    "\n",
    "            # 알파벳 비교 및 처리\n",
    "            if detected_alphabet == target_alphabet:\n",
    "                # 동일한 알파벳: 파란색 상자\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                label = f\"Correct: {detected_alphabet}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            else:\n",
    "                # 다른 알파벳: 빨간색 상자\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                label = f\"Wrong: {detected_alphabet}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "                last_wrong_time = time.time()  # 틀린 시간 기록\n",
    "\n",
    "                # 틀린 메시지 출력\n",
    "                print(f\"틀렸습니다! 목표 알파벳: {target_alphabet}, 감지된 알파벳: {detected_alphabet}\")\n",
    "\n",
    "    # MediaPipe 손 랜드마크 감지\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    # MediaPipe 랜드마크 표시\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "            )\n",
    "\n",
    "    # # 틀렸을 때 빨간색 상자 지속 시간 처리\n",
    "    # if time.time() - last_wrong_time < wrong_display_duration:\n",
    "    #     cv2.putText(frame, \"Wrong Input!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv8 + MediaPipe\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
